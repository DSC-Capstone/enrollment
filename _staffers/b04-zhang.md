---
name: Hao Zhang
email: haz094@ucsd.edu
photo: assets/images/hao-zhang.png
website: https://cseweb.ucsd.edu/~haozhang
domain: B04
title: Developing Open Datasets, Models, Systems, and Evaluation Tools for Large (Language) Models
bio: "Hao Zhang is an Assistant Professor in Halıcıoğlu Data Science Institute and the Department of Computer Science and Engineering at UC San Diego. Before joining UCSD, Hao was a postdoctoral researcher at UC Berkeley working with Ion Stoica (2021 - 2023). Hao completed his Ph.D. in Computer Science at Carnegie Mellon University with Eric Xing (2014 - 2020). During PhD, Hao took on leave and worked for the ML platform startup Petuum Inc (2016 - 2021). Hao's research interest is in the intersection area of machine learning and systems. Hao's past work includes Vicuna, FastChat, Alpa, vLLM, Poseidon, Petuum. Hao’s research has been recognized with the Jay Lepreau best paper award at OSDI’21 and an NVIDIA pioneer research award at NeurIPS’17. Hao also cofounded the company LMNet.ai (2023) which has joined Snowflake since November 2023, and the nonprofit LMSYS Org (2023) which maintains many popular open models, evaluation, and systems."
description: "The rapid advancement of large multimodal models has revolutionized AI systems, resulting in unprecedented levels of intelligence as seen in OpenAI’s GPT-4. However, despite its performance, the training and architecture details of GPT-4 remain unclear, hindering research and open-source innovation in this field.
In this project, we'll explore three relevant areas to LLMs:<br>
- On the system side: infrastructure for scalable training and high-throughput serving with advanced memory management and parallelization techniques.<br>
- On the model side, build multimodal model close to ChatGPT quality, which can also interact with the real world by taking actions and using tools.<br>
- On the data and benchmark side, develop a highly curated data sets and benchmark platform with novel data augmentation, data filtering, and ranking methods.<br>"
summer: "Reading LLM papers, get familiar with tools like huggingface, pytorch, FSDP, Megatron-LM, Deepspeed."
oldstudent: https://weiyueli7.github.io/SON/
prerequisites: DSC102, DSC 140A
time: Monday or Wednesday Afternoon, In-Person
style: I am hands-off. I will ask my students to help on some coding details.
seats: 6
tag: Language Models
---
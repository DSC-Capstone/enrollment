---
dual: True
name1: Yusu Wang
email1: yusuwang@ucsd.edu
photo1: assets/images/yusu.jpeg
website1: yusu.belkin-wang.org


name2: Gal Mishne
email2: gmishne@ucsd.edu
photo2: assets/images/mishne.jpeg
website2: http://mishne.ucsd.edu/
domain: A16
title: How Effective are Transformer Based Graph Learning Models? 
bio: "Yusu Wang is a professor in HDSI. She is primarily interested in geometric and topological data analysis, especially graph learning, geometric deep learning, and so on. In general, she would like to develop efficient and effective learning models for complex data, and graphs (as well as point clouds data) constitute one particular type of data that she is interested in.
<br><br>
Gal Mishne is an assistant professor in HDSI. Her research is on geometric data analysis and  focuses on modeling data as lying on a graph or being sampled from a (nonlinear) manifold. Her research group develops methods that take this geometry into account in order to process, analyze, and visualize high-dimensional data. She primarily collaborates with neuroscientists and other biomedical researchers, to apply models and methods to real-world data.
 "
description: "Graph data are ubiquitous across a broad range of applications in science and engineering. In recent years, there has been a tremendous amount of development in efficient neural network models for learning and optimization on graphs. Two families of popular models are: Message passing graph neural networks, and Graph transformer-based models. In particular, with the success of transformer architectures in many other types of data, especially in large language models, it is natural to ask whether graph-transformers can achieve a similar success. On the other hand, originally transformers are not defined for graph data, and in order to use them to handle graph (or point-cloud data), one has to inject graph topology into the model in some way. The ultimate goal of this project is to explore the relative pros and cons of using different graph transformer models for graph learning tasks. In Quarter 1, the students will get familiar with several baseline graph transformer models and understand the underlying principles. In Quarter 2, they will work on different (potentially novel) ways to inject graph information to the transformer and compare the performance, and potentially apply to novel data sets and tasks. 
"
summer: "Please check out pytorch geometric (https://pytorch-geometric.readthedocs.io/en/latest/) on graph learning models, read about simple models such as GCN and GAT.
"
oldstudent: https://drug-repurposing-gnn.github.io/Drug-Repurposing-Website/
prerequisites: Students should already have experience with neural network models (e.g., CNNs, RNNs, or best but not required, GNNs). Solid knowledge of linear algebra and graph theory is preferred.
time: Wednesday 9-10AM, In-Person, 455 HDSI
ta: Zimo Wang
style: We expect students to be self motivated to do the reading and coding tasks in Q1 and to take ownership of their projects in Q2, with our support. Students are expected to treat the project seriously and devote sufficient time to making weekly progress toward their goals. We are always happy to discuss and help problem-solve.
seats: 10
tag: Theoretical Foundations
---

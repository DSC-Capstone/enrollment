---
name: Arya Mazumdar
email: amazumdar@ucsd.edu
photo: assets/images/arya.jpeg
website: https://mazumdar.ucsd.edu/
domain: B03
title: Quantization and Sparsification in LLMs
bio: "Arya Mazumdar is an Associate Professor of Data Science in UC San Diego. He is the Deputy Director and the Associate Director for Research in the NSF AI Institute TILOS, and also the UCSD Site-Lead of NSF TRIPODS Institute EnCORE. Arya obtained his Ph.D. degree from University of Maryland, College Park specializing in information theory. Subsequently Arya was a postdoctoral scholar at Massachusetts Institute of Technology, an assistant professor in University of Minnesota, and an assistant followed by associate professor in University of Massachusetts Amherst.  Arya is a recipient of a Distinguished Dissertation Award for his Ph.D. thesis, the NSF CAREER award, an EURASIP Best Paper Award, and the ISIT Jack K. Wolf Student Paper Award. He is also a Distinguished Lecturer of the IEEE Information Theory Society, 2023-24. He is currently serving as an Associate Editor for the IEEE Transactions on Information Theory and as an Area editor for Now Publishers Foundation and Trends in Communication and Information Theory. Aryaâ€™s research interests include  information theory, coding theory, statistical learning and optimization."
description: "How do you quantize the trained weights of a neural network to get fast inference in a large scale machine learning model? Should you just use the same quantizer for all layers? How do you develop the theory for  quantization in LLMs? How do you quantize the trained weights of a neural network to get fast inference in a large scale machine learning model? Should you just use the same quantizer for all layers? How do you develop the theory for  quantization in LLMs?"
summer: "Look up quantization in deep learning"
oldstudent: https://xianyingkong.github.io/diffusion-text-generation/
prerequisites: Some course on Machine Learning 
time: Tuesday 11AM-12PM, In-Person, 336 HDSI
ta: Zimo Wang
style: Plan to involve my PhD students/postdocs this time
seats: 6
tag: Language Models
---

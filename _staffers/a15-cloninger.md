---
dual: True
name1: Rayan Saab
email1: rsaab@ucsd.edu
photo1: assets/images/rayan.jpeg
website1: math.ucsd.edu/~rsaab, 


name2: Alex Cloninger
email2: acloninger@ucsd.edu
photo2: assets/images/cloninger.jpeg
website2: https://sites.google.com/ucsd.edu/alexandercloninger/home

domain: A15
title: Neural Network Compression with Error Guarantees
bio: "Alex Cloninger is an Associate Professor in Mathematics and the Halicioglu Data Science Institute. He works on computational models for learning similarities between data, and using these similarity measures to solve various scientific problems. Find out more about Dr. Cloninger's research: https://ccom.ucsd.edu/~acloninger/index.html
<br><br>
Rayan Saab is a Professor in the Mathematics Department and at the Halicioglu Data Science Institute. He works on developing computational methods and theory for solving problems related to collecting, processing, and analyzing data. He came to this work first through an undergrad degree in electrical engineering and finding himself always interested in both making things work and understanding why they do. Find out more about Dr. Saab's research: http://www.math.ucsd.edu/~rsaab/"
description: "While deep learning systems can solve a large number of problems, their ever-growing computational demands pose a challenge for deployment on resource-constrained platforms such as cell phones and small chips. To facilitate moving ML systems to edge computing, it is imperative to reduce their number of active weights and to reduce the computational load associated with them. A number of approaches are plausible. Among these, sparsification seeks to reduce the number of active weights by setting as many of them to zero as possible, while quantization replaces, for example, 32-bit floating point weights with weights that require many fewer bits to store. Despite numerous proposed methods for weight sparsification and quantization, only a handful offer theoretical guarantees that predictions using quantized weights will closely approximate those of the original large network.
<br><br>
The domain of this project will cover methods of sparsification and quantization that guarantee the resulting error will remain small, alongside examining their applications across diverse deep learning architectures, including those for computer vision and large language models. It will involve a balanced mixture of theory and practice. Students who choose this project will delve into the mathematical and computational principles behind quantization, utilizing concepts from signal processing, statistical learning, and linear algebra.  They will also engage in hands-on coding and experimentation on algorithms for compressing deep learning models, testing them on various data sets and signal models."
summer: " Here are some relevant readings. Students need not go into the mathematical details of the papers as we can go through them together, but these papers give an idea of the domain. The more familiar you are with the topic, the more we can do!
<br><br>
https://arxiv.org/abs/2201.11113<br>
https://proceedings.mlr.press/v119/elthakeb20a.html<br>
https://arxiv.org/abs/2210.17323
<br>
To be able to obtain really nice experimental results, you'll need to pick up PyTorch and work with the ImageNet dataset. "
oldstudent: https://dsc180-b11-2.github.io/layout-grounded-optimization/
prerequisites: Being comfortable with probability and linear algebra (or willingness to catch up quickly) would be very helpful, as would be a basic familiarity with neural networks. 
time: Monday 2-3PM, In-Person
style: We both are relatively hands-on in the sense that we make ourselves available for problem-solving and discussions. That said, students have to be self-motivated, and motivated to do the readings and the work.
seats: 8
tag: Theoretical Foundations
---
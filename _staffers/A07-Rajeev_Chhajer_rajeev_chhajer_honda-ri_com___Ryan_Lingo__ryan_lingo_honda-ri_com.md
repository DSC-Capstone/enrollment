---
name: Rajeev Chhajer, Ryan Lingo
email: rajeev_chhajer@honda-ri.com, ryan_lingo@honda-ri.com
photo: 
website: https://www.linkedin.com/in/rajeevchhajer/, https://www.linkedin.com/in/ryan-lingo/
domain: A07
title: Evaluation Strategies for Next-Generation AI Systems
bio: |
  Rajeev Chhajer is the Chief Engineer at Honda Research Institute USA and leads the Software-defined Intelligence team at 99P Labs. He is a founding member of 99P Labs, a research initiative dedicated to developing sustainable technologies and innovative approaches to global challenges. His research focuses on smart city ecosystems, embedded systems, and connectivity to support sustainable and efficient mobility.

  Ryan Lingo is an Applied AI Research Engineer and Developer Advocate at 99P Labs. His work focuses on intelligent systems, with research interests in large language models, synthetic data, and applied machine learning. He has an academic background in philosophy and has held roles in data science and software engineering, with experience spanning academic research, industry, and early-stage startups.
description: |
  As AI systems powered by Large Language Models (LLMs) become increasingly pivotal in high-impact scenarios, conventional metrics such as accuracy on public datasets are no longer sufficient. This domain revolves around exploring more flexible, context-aware methodologies for evaluating system performance across diverse dimensions, including depth of reasoning, ethical alignment, user experience, and practical effectiveness. Students will investigate and prototype varied assessment methods in multiple use cases and data environments, potentially incorporating automated audits to ensure continuous model alignment with organizational norms. By designing scalable evaluation pipelines and adapting them to specific application areas, participants will work toward developing AI systems that are more transparent, responsive, and dependable in evolving real-world contexts.
summer: |
  Before the quarter starts, if you are interested in getting a head start, these three works offer advanced perspectives on evaluating large language models:

  • [Your AI Product Needs Evals](https://hamel.dev/blog/posts/evals/) by Hamel Husain  
  This work dives into methods for assessing LLMs with a more holistic lens—addressing performance from qualitative angles that go beyond standard metrics. It challenges us to think about aspects such as contextual coherence and adaptability.

  • [A Field Guide to Rapidly Improving AI Products](https://hamel.dev/blog/posts/field-guide/) by Hamel Husain  
  This guide provides practical strategies and frameworks for developing tailored evaluation pipelines. It emphasizes systematic, reproducible methods that capture nuanced performance characteristics and are well-suited for real-world scenarios.

  • [Task-Specific LLM Evals that Do & Don't Work](https://eugeneyan.com/writing/evals/) by Eugene Yan  
  This piece critically examines traditional evaluation metrics and offers alternative, context-sensitive approaches to measuring LLM performance. It highlights the importance of incorporating reasoning depth, fairness, and other ethical considerations into our testing frameworks.

  These resources together reinforce a shift toward evaluation methods that are context-aware, scalable, and aligned with the intricacies of modern language models. Focusing on these ideas will help you design and prototype robust evaluation pipelines that can address the evolving challenges in AI.

  If you'd like to discuss the project further, please connect on LinkedIn with Ryan Lingo; he would be happy to discuss further.
time: Mondays at 12pm-1pm PST
ta: Zoom
style: |
  We plan to take an engaged but student-led approach to mentoring. We'll work closely with the students throughout the project—meeting regularly, providing guidance, and being available for feedback and support. That said, we're looking for high-agency students who are excited to take ownership of their learning and direction.

  The best mental model for this capstone is "learning in public." Students will play an active role in shaping the plan and setting objectives. Rather than being given step-by-step instructions, they'll be encouraged to explore, make decisions, and figure out how to execute their ideas, with our mentorship to guide the way. We'll help them think critically, problem-solve, and communicate their process and outcomes clearly.

  While we won’t dictate tasks at a granular level, we’ll be present every week and ensure they have the support and structure they need to succeed.
seats: 12
tag: llm
---


---
name: Alex Warstadt
email: awarstadt@ucsd.edu
photo: 
website: https://alexwarstadt.github.io
domain: 
title: Training Baby Language Models from Scratch
bio: Alex Warstadt is an Assistant Professor at UCSD with joint appointments in HDSI and the Department of Linguistics. He received his PhD in linguistics in 2022 from NYU under Samuel Bowman, and completed a postdoc in 2024 with Ryan Cotterell. Alex runs UCSD's Learning Meaning and Natural Language Lab (LeMN Lab) which is an interdisciplinary group that uses insights from linguistics to advance and interpret language models and uses advances in machine learning to answer scientific questions about the nature of language.
description: Large Language Models have an impressive ability to learn and use human language, but humans are still the state-of-the-art when it comes to learning language efficiently. We acquire language from 100 million words or less, whereas LLMs are now trained on 10s or *trillions* of words. The BabyLM Challenge (https://babylm.github.io/) is a competition centered around training small "BabyLMs" under constraints inspired by human language learning. The goal of a BabyLM submission is to train a model that learns language as data-efficiently as a human or that simulates properties of human learning and linguistic performance.
summer: Replicate prior BabyLM studies, including training LMs from scratch and evaluating, launching jobs on a UCSD supercomputer cluster such as Nautilus, and tracking training progress and sweeps over several days or weeks using weights and biases.
time: Wednesday afternoon??
modality: In-person (need room booked for me; required if mentoring >4 students in-person)
ta: TBD
style: This topic lends itself to multiple parallel projects, but a single large project is acceptable as well depending on group size. PhD students in LeMN Lab will be encouraged to co-supervise  projects.
seats: 6
tag: llm

---
